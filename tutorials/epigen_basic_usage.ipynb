{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial explains how to run EpiGen inference, annotate phenotype-associated TCRs / epitopes, and take ensemble. The overall process is as follows:\n",
    "\n",
    "**inference --> annotation --> ensemble**\n",
    "\n",
    "To start with, prepare an input file as in `data/sample_tcrs.csv` that contains CDR3b sequences in the 'tcr' column. Download the epitope database from https://zenodo.org/records/14624873/files/tumor_associated_epitopes.csv?download=1. Let's assume we place it under the `data/` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cd EpiGen\n",
    "mkdir data && cd data\n",
    "wget https://zenodo.org/records/14624873/files/tumor_associated_epitopes.csv\n",
    "wget https://zenodo.org/records/14861398/files/obs_annotated_cancer_wu_ens_th0.5.csv\n",
    "wget https://zenodo.org/records/14861864/files/sample_tcrs.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to determine other high-level parameters such as `top_k`, `method`, `ens_th`, and `tokenizer_path`. \n",
    "\n",
    "- `top_k` is the number of epitopes generated from each TCR. Although TCR-pMHC interaction is highly specific, it is estimated that one TCR should be able to recognize multiple epitopes (ref: A. K. Sewell, Why must T cells be cross-reactive?, Nature Reviews Immunology, 2012). This is a hyper-parameter that should be determined considering the characteristics of the dataset you're dealing with. In our manuscript, we set top_k=1 for the cancer dataset analysis, and top_k=8 for the COVID-19 dataset analysis. This is because of the number of PA T cells detected. COVID-19 is a single species where there are only ~1,500 CD8+ T cell epitopes. The database of tumor-associated epitopes is much larger. For more details, please refer to Supplementary Note 4 of the manuscript. \n",
    "\n",
    "- `method` is the matching method to be used for querying the epitopes to the epitope database. The current code supports two methods: 'substring' and 'levenshtein'. 'levenshtein' can be used together with a threshold which may avoid from strict matching. 'substring' is much faster and is recommended for the first run, because in our experience they are not too different. \n",
    "\n",
    "- `ens_th` is the threshold for the ensembling multiple annotation files. The default mode runs inference using 11 independent models. `ens_th` of 0.5 means a TCR is considered PA if it is predicted as PA by at least 6 models (> 0.5 of the total models). You may want to tune this sometimes as well to get more robust results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the above, let's define a simple configuration in python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"exp_dir\": \"example_run\",\n",
    "    \"input_file\": \"data/sample_tcrs.csv\",\n",
    "    \"epitope_db\": \"data/tumor_associated_epitopes.csv\",\n",
    "    \"top_k\": 4,\n",
    "    \"method\": 'substring',\n",
    "    \"ens_th\": 0.5,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, import some python modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from epigen import EpiGenPredictor,EpitopeAnnotator,EpitopeEnsembler,visualize_match_overlaps_parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, read in the data and run inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read the input data\n",
    "tcrs = pd.read_csv(cfg['input_file'])\n",
    "tcrs = tcrs[\"text\"].tolist()\n",
    "\n",
    "# Predict from TCR sequences\n",
    "predictor = EpiGenPredictor()\n",
    "results = predictor.predict_all(\n",
    "    tcr_sequences=tcrs,\n",
    "    output_dir=f\"{cfg['exp_dir']}/predictions\",\n",
    "    top_k=cfg['top_k']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create 'predictions' under 'example_run' directory. You need GPUs with more than 24GB memory to run this (to hold GPT2-small architecture). Here, `predict_all_models()` function is a wrapper to `predict()` function, which runs inference using 11 independent models in sequence. If you want to speed up the process, you may want to run inference in parallel using the `predict()` function. Running the above code would have created `example_run/predictions/predictions_{i}.csv` files that contains the generated epitopes in the 'pred_{i}' columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the annotation step using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Annotate phenotype-associated epitopes / tcrs\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "annotator = EpitopeAnnotator(cfg['epitope_db'])\n",
    "annotator.annotate_all(\n",
    "    predictions_dir=f\"{cfg['exp_dir']}/predictions\",\n",
    "    output_dir=f\"{cfg['exp_dir']}/annotations\",\n",
    "    top_k=cfg['top_k'],\n",
    "    method=cfg['method']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step does not use GPUs but may take a while depending on the size of the dataset. This will create 'annotations' under 'example_run' directory. Multiprocessing is used to speed up the process. Running the above code would have created `example_run/annotations/annotations_{i}.csv` files that contains the annotated epitopes in the 'match_{i}', 'ref_epitope_{i}', and 'ref_protein_{i}' columns. For a TCR, match_1==1 means pred_1 (epitope) was found to match an entry in the database and labeled as PA. \n",
    "\n",
    "Here, you may check some of the agreements between the eleven models by runing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "annotation_files = [f\"{cfg['exp_dir']}/annotations/annotations_{model_idx}.csv\" for model_idx in range(1, 1 + 11)]\n",
    "# Visualize the match overlaps between 11 annotations\n",
    "similarity_matrix, file_names = visualize_match_overlaps_parallel(\n",
    "    files_list=annotation_files,\n",
    "    outdir=f\"{cfg['exp_dir']}/ensemble\",\n",
    "    top_k=cfg['top_k'],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the agreements, you may want to tune the `ens_th` and `top_k` parameters. \n",
    "\n",
    "Finally, run the ensemble step using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble 11 annotations to get the final robust annotation\n",
    "ensembler = EpitopeEnsembler(threshold=cfg['ens_th'])\n",
    "final_results = ensembler.ensemble(\n",
    "    annotation_files,\n",
    "    output_path=f\"{cfg['exp_dir']}/ensemble/annotations_ens_all_th{cfg['ens_th']}.csv\",\n",
    "    top_k=cfg['top_k']\n",
    ")\n",
    "\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above code create 'ensemble' under 'example_run' directory. This is the final annotation file that contains the ensembled results. Please repeat until here at least five times to get multiple annotation files. You'll want to check the consistency of the results later. As EpiGen called model.generate() function of GPT-2, it generates different epitope sequences for the same TCR when it is run again. This is why we adopted taking an ensemble of 11 different models, which resulted in robust result in our dataset. \n",
    "\n",
    "You may merge this with your single-cell transcriptomics data to analyze the TCR-phenotype associations. From this point, the functions needed would be different by the structure of your dataset. Here, as an example, download an annotated observation file from https://zenodo.org/records/14624873/files/obs_annotated_cancer_wu_ens_th0.5.csv (under `data`) where we'll inject our TCR epitope annotation information. Let's consider a utility function to merge this with the single-cell data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def merge_annotations(\n",
    "    site_file: str,\n",
    "    annotation_file: str,\n",
    "    output_dir: str = \"merged\",\n",
    "    randomize: bool = False,\n",
    "    random_seed: int = 42\n",
    "    ):\n",
    "    \"\"\"Merge site data with new annotations by matching TCR sequences.\n",
    "\n",
    "    Args:\n",
    "        site_file: Path to site_added.csv\n",
    "        annotation_file: Path to annotation_ens_th0.5.csv\n",
    "        output_dir: Directory to save output file\n",
    "        randomize: Whether to randomize annotation matches\n",
    "        random_seed: Seed for reproducible randomization\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Starting Annotation Merge ===\")\n",
    "    print(f\"• Mode: {'Randomized' if randomize else 'Normal'}\")\n",
    "\n",
    "    # Read input files\n",
    "    print(\"• Reading input files...\")\n",
    "    site_df = pd.read_csv(site_file)\n",
    "    annot_df = pd.read_csv(annotation_file)\n",
    "\n",
    "    if randomize:\n",
    "        print(\"• Randomizing annotation matches...\")\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        # Identify columns to shuffle\n",
    "        match_cols = [col for col in annot_df.columns if any(x in col for x in ['match_', 'ref_epitope_', 'ref_protein_'])]\n",
    "\n",
    "        # Group columns by their index (e.g., match_0, ref_epitope_0, ref_protein_0)\n",
    "        col_groups = {}\n",
    "        for col in match_cols:\n",
    "            idx = col.split('_')[-1]\n",
    "            if idx.isdigit():\n",
    "                if idx not in col_groups:\n",
    "                    col_groups[idx] = []\n",
    "                col_groups[idx].append(col)\n",
    "\n",
    "        # Shuffle each group of columns together\n",
    "        for idx, cols in col_groups.items():\n",
    "            shuffle_idx = np.random.permutation(len(annot_df))\n",
    "            annot_df[cols] = annot_df[cols].iloc[shuffle_idx].values\n",
    "\n",
    "                # Get columns to keep from site_df\n",
    "    keep_cols = []\n",
    "    drop_patterns = ['pred_', 'ref_epitope_', 'ref_protein_', 'match_']\n",
    "    for col in site_df.columns:\n",
    "        if not any(pattern in col for pattern in drop_patterns):\n",
    "            keep_cols.append(col)\n",
    "\n",
    "    # Create clean site dataframe\n",
    "    print(\"• Removing old predictions and annotations...\")\n",
    "    site_clean = site_df[keep_cols].copy()\n",
    "\n",
    "    # Rename columns for merging\n",
    "    annot_df = annot_df.rename(columns={'tcr': 'cdr3'})\n",
    "\n",
    "    # Merge dataframes\n",
    "    print(\"• Merging with new annotations...\")\n",
    "    merged_df = site_clean.merge(annot_df, on='cdr3', how='left')\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Generate output filename\n",
    "    site_stem = Path(site_file).stem\n",
    "    annot_stem = Path(annotation_file).stem\n",
    "    random_suffix = \"_randomized\" if randomize else \"\"\n",
    "    output_file = output_dir / f\"{site_stem}_merged_{annot_stem}{random_suffix}.csv\"\n",
    "\n",
    "    # Save merged file\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"\\n=== Merge Summary ===\")\n",
    "    print(f\"• Total cells in site file: {len(site_df)}\")\n",
    "    print(f\"• Total TCRs in annotation file: {len(annot_df)}\")\n",
    "    print(f\"• Cells matched with annotations: {merged_df['pred_0'].notna().sum()}\")\n",
    "    print(f\"• Cells without matches: {merged_df['pred_0'].isna().sum()}\")\n",
    "\n",
    "    # Print match statistics\n",
    "    match_cols = [col for col in merged_df.columns if col.startswith('match_')]\n",
    "    for k in range(min(4, len(match_cols))):  # Show first 4 positions\n",
    "        matches = merged_df[f'match_{k}'].sum()\n",
    "        total = merged_df[f'match_{k}'].notna().sum()\n",
    "        if total > 0:\n",
    "            print(f\"• Match rate at k={k}: {matches/total*100:.1f}%\")\n",
    "\n",
    "    print(f\"\\n• Results saved to: {output_file}\")\n",
    "    print(\"===========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function with the site file and the annotation file to inject our epitope annotation information to the transcriptomics data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### Merge old code's annotation to cancer_wu site_added.csv\n",
    "merged_df = merge_annotations(\n",
    "    site_file=f\"data/obs_annotated_cancer_wu_ens_th0.5.csv\",\n",
    "    annotation_file=f\"{cfg['exp_dir']}/ensemble/annotations_ens_all_th{cfg['ens_th']}.csv\",\n",
    "    output_dir=f\"{cfg['exp_dir']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above code creates the merged file under the `example_run` directory. We can now use this file to analyze the TCR-phenotype associations. Download the `cancer_wu` dataset to get the more raw transcriptomics data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from research.cancer_wu.download import download_and_preprocess\n",
    "download_and_preprocess(outdir=\"data/cancer_wu\", input_file=\"research/cancer_wu/data_links.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will download the `cancer_wu` dataset from https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE139555. We need to define some utility functions to read in the transcriptomics data. Please refer to `research/cacner_wu/analyze.py` and `research/cancer_wu/utils.py` for more details. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# from research.cancer_wu.utils import *\n",
    "# from research.cancer_wu.analyze import *\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import os\n",
    "import anndata as ad\n",
    "import scirpy as ir\n",
    "import mudata as md\n",
    "from mudata import MuData\n",
    "\n",
    "CELL_TYPES = ['8.1-Teff', '8.2-Tem', '8.3a-Trm', '8.3b-Trm', '8.3c-Trm']\n",
    "SAMPLES = ['CN1', 'CN2', 'CT1', 'CT2', 'EN1', 'EN2', 'EN3', 'ET1', 'ET2', 'ET3',\n",
    "           'LB6', 'LN1', 'LN2', 'LN3', 'LN4', 'LN5', 'LN6', 'LT1', 'LT2', 'LT3',\n",
    "           'LT4', 'LT5', 'LT6', 'RB1', 'RB2', 'RB3', 'RN1', 'RN2', 'RN3', 'RT1',\n",
    "           'RT2', 'RT3']\n",
    "           \n",
    "def read_tcell_integrated(data_dir, transpose=False):\n",
    "    \"\"\"\n",
    "    Read the main gene expression data\n",
    "    \"\"\"\n",
    "    # Read the H5AD file\n",
    "    adata = sc.read_h5ad(f\"{data_dir}/GSE139555_tcell_integrated.h5ad\")\n",
    "    if transpose:\n",
    "        adata = adata.transpose()\n",
    "    metadata = pd.read_csv(f\"{data_dir}/GSE139555%5Ftcell%5Fmetadata.txt\", sep=\"\\t\", index_col=0)\n",
    "    # Make sure the index of the metadata matches the obs_names of the AnnData object\n",
    "    adata.obs = adata.obs.join(metadata, how='left')\n",
    "    print(\"Successfully read GSE139555_t_cell_integrated!\")\n",
    "    return adata\n",
    "\n",
    "\n",
    "def read_all_data(data_dir, obs_cache=None, filter_cdr3_notna=True, filter_cell_types=True):\n",
    "    \"\"\"\n",
    "    The main function to read CD8+ T cell data from Wu et al. dataset\n",
    "    Both gene expression and TCR sequences are read\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_dir: str\n",
    "        Root directory of the data\n",
    "    obs_cache: str / None\n",
    "        csv file that contains some annotated TCR data. As there are multiple annotation steps,\n",
    "        this file is always read after the very first annotation\n",
    "    filter_cdr3_notna: bool\n",
    "        Drop the rows that do not have viable CDR3 sequence information\n",
    "    filter_cell_types: bool\n",
    "        Drop the rows that are not CD8+ T cells\n",
    "    \"\"\"\n",
    "    samples = ['CN1', 'CT2', 'EN3', 'ET3', 'LB6', 'LN3', 'LN6', 'LT3', 'LT6', 'RB2', 'RN2', 'RT2',\n",
    "               'CN2', 'EN1', 'ET1', 'LN1', 'LN4', 'LT1', 'LT4', 'RB3', 'RN3', 'RT3',\n",
    "               'CT1', 'EN2', 'ET2', 'LN2', 'LN5', 'LT2', 'LT5', 'RB1', 'RN1', 'RT1']\n",
    "    # Read T-cell integrated (gene expression data)\n",
    "    adata = read_tcell_integrated(data_dir)\n",
    "\n",
    "    # Read the TCR sequencing data using scirpy (ir)\n",
    "    airrs = []\n",
    "    for sample in [s for s in os.listdir(data_dir) if s in samples]:\n",
    "        for x in os.listdir(f\"{data_dir}/{sample}\"):\n",
    "            if x.endswith(\"contig_annotations.csv\") or x.endswith(\"annotations.csv\"):\n",
    "                airr = ir.io.read_10x_vdj(f\"{data_dir}/{sample}/{x}\")\n",
    "                # Add a column to identify the source file\n",
    "                airr.obs['new_cell_id'] = airr.obs.index.map(lambda x: sample + \"_\" + x)\n",
    "                airr.obs.index = airr.obs['new_cell_id']\n",
    "                airrs.append(airr)\n",
    "    # Merge the AIRR objects\n",
    "    if len(airrs) > 1:\n",
    "        merged_airr = ad.concat(airrs)\n",
    "    else:\n",
    "        merged_airr = airrs[0]\n",
    "\n",
    "    if obs_cache:\n",
    "        print(f\"Reading cache from {obs_cache}..\")\n",
    "        df_cache = pd.read_csv(obs_cache)\n",
    "\n",
    "        # Merge df_cache to adata.obs based on cell_id\n",
    "        # Set cell_id as index in df_cache to match adata.obs\n",
    "        df_cache = df_cache.set_index('cell_id')\n",
    "\n",
    "        # Keep only the cells that exist in df_cache\n",
    "        common_cells = adata.obs.index.intersection(df_cache.index)\n",
    "        adata = adata[common_cells].copy()\n",
    "\n",
    "        # Update adata.obs with all columns from df_cache\n",
    "        # This will overwrite existing columns and add new ones\n",
    "        adata.obs = adata.obs.combine_first(df_cache)\n",
    "\n",
    "        # For columns that exist in both, prefer df_cache values\n",
    "        for col in df_cache.columns:\n",
    "            if col in adata.obs:\n",
    "                adata.obs[col] = df_cache[col]\n",
    "\n",
    "        print(f\"Updated adata.obs with {len(df_cache.columns)} columns from cache\")\n",
    "        print(f\"Retained {len(common_cells)} cells after matching with cache\")\n",
    "\n",
    "    if filter_cell_types:\n",
    "        print(\"Get only CD8+ T cells..\")\n",
    "        adata = adata[adata.obs['ident'].isin(CELL_TYPES)].copy()\n",
    "\n",
    "    if filter_cdr3_notna:\n",
    "        # Filter based on non-NA cdr3 values:\n",
    "        valid_cells = adata.obs['cdr3'].notna()\n",
    "        print(f\"Filtering out {(~valid_cells).sum()} cells with NA cdr3 values\")\n",
    "        adata = adata[valid_cells].copy()\n",
    "\n",
    "    mdata = MuData({\"airr\": merged_airr, \"gex\": adata})\n",
    "\n",
    "    print(f\"Successfully merged {len(airrs)} AIRR objects!\")\n",
    "    print(f\"(read_all_data) The number of CD8+ T cells: {len(adata.obs)}\")\n",
    "    return mdata\n",
    "\n",
    "\n",
    "def read_all_raw_data(data_dir):\n",
    "    samples = os.listdir(data_dir)\n",
    "    adata_list = []\n",
    "\n",
    "    for sample in samples:\n",
    "        if sample in SAMPLES:\n",
    "            sample_path = os.path.join(data_dir, sample)\n",
    "            file = os.listdir(sample_path)[0]\n",
    "\n",
    "            # Read the data with the prefix applied to barcodes\n",
    "            adata = sc.read_10x_mtx(\n",
    "                path=sample_path,\n",
    "                var_names=\"gene_symbols\",\n",
    "                make_unique=True,\n",
    "                prefix=file.split(\".\")[0] + \".\"\n",
    "            )\n",
    "\n",
    "            # Rename the barcodes\n",
    "            prefix = f\"{sample}_\"\n",
    "            adata.obs_names = [f\"{prefix}{barcode}\" for barcode in adata.obs_names]\n",
    "\n",
    "            # Append the annotated data to the list\n",
    "            adata_list.append(adata)\n",
    "\n",
    "    # Concatenate all the data into one AnnData object\n",
    "    combined_adata = ad.concat(adata_list, axis=0)\n",
    "    print(\"Successfully read all RAW data!\")\n",
    "\n",
    "    return combined_adata\n",
    "\n",
    "\n",
    "def filter_and_update_combined_adata(combined_adata, processed_adata):\n",
    "    # Get the common indices (barcodes) between the combined_adata and processed_adata\n",
    "    common_indices = processed_adata.obs_names.intersection(combined_adata.obs_names)\n",
    "\n",
    "    # Filter combined_adata to keep only those cells present in processed_adata\n",
    "    filtered_combined_adata = combined_adata[common_indices].copy()\n",
    "\n",
    "    # Copy obs from processed_adata to filtered_combined_adata\n",
    "    for col in processed_adata.obs.columns:\n",
    "        # Add a new column in filtered_combined_adata if it doesn't already exist\n",
    "        if col not in filtered_combined_adata.obs.columns:\n",
    "            filtered_combined_adata.obs[col] = None\n",
    "\n",
    "        # Copy the data from processed_adata.obs to filtered_combined_adata.obs, matching by index\n",
    "        filtered_combined_adata.obs[col] = processed_adata.obs.loc[common_indices, col]\n",
    "\n",
    "    print(f\"Filtered the combined data using the processed adata! (Finding intersection). Num of rows={len(filtered_combined_adata)}\")\n",
    "\n",
    "    return filtered_combined_adata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's run the differential gene expression analysis between Phenotype-Associated (PA) T cell that we marked by match_{i} to be 1 and other background T cells. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from epigen import DEGAnalyzer\n",
    "# Read the processed gene expression data of CD8+ T cell and then inject our epitope annotation\n",
    "mdata = read_all_data(data_dir=\"data/cancer_wu\", obs_cache=f\"{cfg['exp_dir']}/obs_annotated_cancer_wu_ens_th0.5_merged_annotations_ens_all_th0.5.csv\")\n",
    "# Read the raw gene expression data of CD8+ T cells\n",
    "raw_adata = read_all_raw_data(data_dir=\"data/cancer_wu\")\n",
    "# Merge the raw gene expression data with the previous TCR-GEX data\n",
    "raw_adata_filtered = filter_and_update_combined_adata(raw_adata, mdata['gex'])\n",
    "\n",
    "# Perform DEG analysis\n",
    "for k in range(1, 1 + cfg['top_k']):\n",
    "    analyzer = DEGAnalyzer(output_dir=f\"{cfg['exp_dir']}/gex_grouped\", top_k=k)\n",
    "    analyzer.analyze(raw_adata_filtered.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more thorough result, please use a complete data instead of sample_tcrs.csv. Also, run the pipeline multiple times (prediction, annotation, ensemble, ..). "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
