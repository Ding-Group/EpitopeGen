{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial explains how to run EpiGen inference, annotate phenotype-associated TCRs / epitopes, and take ensemble. The overall process is as follows:\n",
    "\n",
    "**inference --> annotation --> ensemble**\n",
    "\n",
    "To start with, prepare an input file as in `data/sample_tcrs.csv` that contains CDR3b sequences in the 'tcr' column. Download the epitope database from https://zenodo.org/records/14624873/files/tumor_associated_epitopes.csv?download=1. Let's assume we place it under the `data/` directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "mkdir data && cd data\n",
    "wget https://zenodo.org/records/14624873/files/tumor_associated_epitopes.csv\n",
    "wget https://zenodo.org/records/14861398/files/obs_annotated_cancer_wu_ens_th0.5.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we need to determine other high-level parameters such as `top_k`, `method`, `ens_th`, and `tokenizer_path`. \n",
    "\n",
    "- `top_k` is the number of epitopes generated from each TCR. Although TCR-pMHC interaction is highly specific, it is estimated that one TCR should be able to recognize multiple epitopes (ref: A. K. Sewell, Why must T cells be cross-reactive?, Nature Reviews Immunology, 2012). This is a hyper-parameter that should be determined considering the characteristics of the dataset you're dealing with. In our manuscript, we set top_k=1 for the cancer dataset analysis, and top_k=8 for the COVID-19 dataset analysis. This is because of the number of PA T cells detected. COVID-19 is a single species where there are only ~1,500 CD8+ T cell epitopes. The database of tumor-associated epitopes is much larger. For more details, please refer to Supplementary Note 4 of the manuscript. \n",
    "\n",
    "- `method` is the matching method to be used for querying the epitopes to the epitope database. The current code supports two methods: 'substring' and 'levenshtein'. 'levenshtein' can be used together with a threshold which may avoid from strict matching. 'substring' is much faster and is recommended for the first run, because in our experience they are not too different. \n",
    "\n",
    "- `ens_th` is the threshold for the ensembling multiple annotation files. The default mode runs inference using 11 independent models. `ens_th` of 0.5 means a TCR is considered PA if it is predicted as PA by at least 6 models (> 0.5 of the total models). You may want to tune this sometimes as well to get more robust results. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering the above, let's define a simple configuration in python dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "cfg = {\n",
    "    \"exp_dir\": \"example_run\",\n",
    "    \"input_file\": \"data/sample_tcrs.csv\",\n",
    "    \"epitope_db\": \"data/epitopes_db.csv\",\n",
    "    \"top_k\": 4,\n",
    "    \"method\": 'substring',\n",
    "    \"ens_th\": 0.5,\n",
    "    \"tokenizer_path\": 'research/regaler/EpiGen'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, read in the data and run inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read the input data\n",
    "tcrs = pd.read_csv(cfg['input_file'])\n",
    "tcrs = tcrs[\"text\"].tolist()\n",
    "\n",
    "# Predict from TCR sequences\n",
    "predictor = EpiGenPredictor(tokenizer_path=cfg['tokenizer_path'])\n",
    "results = predictor.predict_all(\n",
    "    tcr_sequences=tcrs,\n",
    "    output_dir=f\"{cfg['exp_dir']}/predictions\",\n",
    "    top_k=cfg['top_k']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will create 'predictions' under 'example_run' directory. You need GPUs with more than 24GB memory to run this (to hold GPT2-small architecture). Here, `predict_all_models()` function is a wrapper to `predict()` function, which runs inference using 11 independent models in sequence. If you want to speed up the process, you may want to run inference in parallel using the `predict()` function. Running the above code would have created `example_run/predictions/predictions_{i}.csv` files that contains the generated epitopes in the 'pred_{i}' columns. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, run the annotation step using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Annotate phenotype-associated epitopes / tcrs\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "annotator = EpitopeAnnotator(cfg['epitope_db'])\n",
    "annotator.annotate_all(\n",
    "    predictions_dir=f\"{cfg['exp_dir']}/predictions\",\n",
    "    output_dir=f\"{cfg['exp_dir']}/annotations\",\n",
    "    top_k=top_k,\n",
    "    method=method\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This step does not use GPUs but may take a while depending on the size of the dataset. This will create 'annotations' under 'example_run' directory. Multiprocessing is used to speed up the process. Running the above code would have created `example_run/annotations/annotations_{i}.csv` files that contains the annotated epitopes in the 'match_{i}', 'ref_epitope_{i}', and 'ref_protein_{i}' columns. For a TCR, match_1==1 means pred_1 (epitope) was found to match an entry in the database and labeled as PA. \n",
    "\n",
    "Here, you may check some of the agreements between the eleven models by runing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "annotation_files = [f\"{cfg['exp_dir']}/annotations_{model_idx}.csv\" for model_idx in range(11)]\n",
    "# Visualize the match overlaps between 11 annotations\n",
    "similarity_matrix, file_names = visualize_match_overlaps_parallel(\n",
    "    files_list=annotation_files,\n",
    "    outdir=f\"{cfg['exp_dir']}/ensemble\",\n",
    "    top_k=top_k,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the agreements, you may want to tune the `ens_th` and `top_k` parameters. \n",
    "\n",
    "Finally, run the ensemble step using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Ensemble 11 annotations to get the final robust annotation\n",
    "ensembler = EpitopeEnsembler(threshold=cfg['ens_th'])\n",
    "final_results = ensembler.ensemble(\n",
    "    annotation_files,\n",
    "    output_path=f\"{cfg['exp_dir']}/ensemble/annotations_ens_all_th{cfg['ens_th']}.csv\",\n",
    "    top_k=cfg['top_k']\n",
    ")\n",
    "\n",
    "print(final_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above code create 'ensemble' under 'example_run' directory. This is the final annotation file that contains the ensembled results. Please repeat until here at least five times to get multiple annotation files. You'll want to check the consistency of the results later. As EpiGen called model.generate() function of GPT-2, it generates different epitope sequences for the same TCR when it is run again. This is why we adopted taking an ensemble of 11 different models, which resulted in robust result in our dataset. \n",
    "\n",
    "You may merge this with your single-cell transcriptomics data to analyze the TCR-phenotype associations. As an example, download an annotated observation file from https://zenodo.org/records/14624873/files/obs_annotated_cancer_wu_ens_th0.5.csv (under `data`) where we'll inject our annotation information. Let's consider a utility function to merge this with the single-cell data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def merge_annotations(\n",
    "    site_file: str,\n",
    "    annotation_file: str,\n",
    "    output_dir: str = \"merged\",\n",
    "    randomize: bool = False,\n",
    "    random_seed: int = 42\n",
    "    ):\n",
    "    \"\"\"Merge site data with new annotations by matching TCR sequences.\n",
    "\n",
    "    Args:\n",
    "        site_file: Path to site_added.csv\n",
    "        annotation_file: Path to annotation_ens_th0.5.csv\n",
    "        output_dir: Directory to save output file\n",
    "        randomize: Whether to randomize annotation matches\n",
    "        random_seed: Seed for reproducible randomization\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Starting Annotation Merge ===\")\n",
    "    print(f\"• Mode: {'Randomized' if randomize else 'Normal'}\")\n",
    "\n",
    "    # Read input files\n",
    "    print(\"• Reading input files...\")\n",
    "    site_df = pd.read_csv(site_file)\n",
    "    annot_df = pd.read_csv(annotation_file)\n",
    "\n",
    "    if randomize:\n",
    "        print(\"• Randomizing annotation matches...\")\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "        # Identify columns to shuffle\n",
    "        match_cols = [col for col in annot_df.columns if any(x in col for x in ['match_', 'ref_epitope_', 'ref_protein_'])]\n",
    "\n",
    "        # Group columns by their index (e.g., match_0, ref_epitope_0, ref_protein_0)\n",
    "        col_groups = {}\n",
    "        for col in match_cols:\n",
    "            idx = col.split('_')[-1]\n",
    "            if idx.isdigit():\n",
    "                if idx not in col_groups:\n",
    "                    col_groups[idx] = []\n",
    "                col_groups[idx].append(col)\n",
    "\n",
    "        # Shuffle each group of columns together\n",
    "        for idx, cols in col_groups.items():\n",
    "            shuffle_idx = np.random.permutation(len(annot_df))\n",
    "            annot_df[cols] = annot_df[cols].iloc[shuffle_idx].values\n",
    "\n",
    "                # Get columns to keep from site_df\n",
    "    keep_cols = []\n",
    "    drop_patterns = ['pred_', 'ref_epitope_', 'ref_protein_', 'match_']\n",
    "    for col in site_df.columns:\n",
    "        if not any(pattern in col for pattern in drop_patterns):\n",
    "            keep_cols.append(col)\n",
    "\n",
    "    # Create clean site dataframe\n",
    "    print(\"• Removing old predictions and annotations...\")\n",
    "    site_clean = site_df[keep_cols].copy()\n",
    "\n",
    "    # Rename columns for merging\n",
    "    annot_df = annot_df.rename(columns={'tcr': 'cdr3'})\n",
    "\n",
    "    # Merge dataframes\n",
    "    print(\"• Merging with new annotations...\")\n",
    "    merged_df = site_clean.merge(annot_df, on='cdr3', how='left')\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = Path(output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Generate output filename\n",
    "    site_stem = Path(site_file).stem\n",
    "    annot_stem = Path(annotation_file).stem\n",
    "    random_suffix = \"_randomized\" if randomize else \"\"\n",
    "    output_file = output_dir / f\"{site_stem}_merged_{annot_stem}{random_suffix}.csv\"\n",
    "\n",
    "    # Save merged file\n",
    "    merged_df.to_csv(output_file, index=False)\n",
    "\n",
    "    # Print statistics\n",
    "    print(\"\\n=== Merge Summary ===\")\n",
    "    print(f\"• Total cells in site file: {len(site_df)}\")\n",
    "    print(f\"• Total TCRs in annotation file: {len(annot_df)}\")\n",
    "    print(f\"• Cells matched with annotations: {merged_df['pred_0'].notna().sum()}\")\n",
    "    print(f\"• Cells without matches: {merged_df['pred_0'].isna().sum()}\")\n",
    "\n",
    "    # Print match statistics\n",
    "    match_cols = [col for col in merged_df.columns if col.startswith('match_')]\n",
    "    for k in range(min(4, len(match_cols))):  # Show first 4 positions\n",
    "        matches = merged_df[f'match_{k}'].sum()\n",
    "        total = merged_df[f'match_{k}'].notna().sum()\n",
    "        if total > 0:\n",
    "            print(f\"• Match rate at k={k}: {matches/total*100:.1f}%\")\n",
    "\n",
    "    print(f\"\\n• Results saved to: {output_file}\")\n",
    "    print(\"===========================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Call the function with the site file and the annotation file. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge old code's annotation to cancer_wu site_added.csv\n",
    "merged_df = merge_annotations(\n",
    "    site_file=f\"data/obs_annotated_cancer_wu_ens_th0.5.csv\",\n",
    "    annotation_file=f\"{cfg['exp_dir']}/ensemble/annotations_ens_all_th{cfg['ens_th']}.csv\",\n",
    "    output_dir=f\"{cfg['exp_dir']}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the above code creates the merged file under the `example_run` directory. This file is the observation file in scanpy object. Use this file to analyze the TCR-phenotype associations. Download the `cancer_wu` dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from research.cancer_wu.download import download_and_preprocess\n",
    "download_and_preprocess(outdir=\"data/cancer_wu\", input_file=\"research/cancer_wu/data_links.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will download the `cancer_wu` dataset from https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE139555. Then, run the following to perform basic differential gene expression analysis. Please refer to `research/cacner_wu/analyze.py` for more detail. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from research.cancer_wu.utils import *\n",
    "from research.cancer_wu.analyze import *\n",
    "mdata = read_all_data(data_dir='data/cancer_wu', obs_cache=f\"{cfg['exp_dir']}/obs_annotated_cancer_wu_ens_th0.5_merged_annotations_ens_all_th0.5.csv\")\n",
    "raw_adata = read_all_raw_data(data_dir='data/cancer_wu')\n",
    "raw_adata_filtered = filter_and_update_combined_adata(raw_adata, mdata['gex'])\n",
    "for k in range(1, 1 + cfg['top_k']):\n",
    "    expression_level_analysis_grouped(raw_adata_filtered.copy(), outdir=f\"{cfg['exp_dir']}/gex_grouped\", top_k=k)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
