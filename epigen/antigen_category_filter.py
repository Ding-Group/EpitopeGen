# Standard library imports
import math
import os
import random
import shutil
import subprocess
import tempfile
import time
import xml.etree.ElementTree as ET
from collections import defaultdict
from pathlib import Path
from subprocess import Popen, PIPE
from typing import Callable, Dict

# Third-party library imports
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from tqdm import tqdm

# Local codes
from epigen.utils import measure_time

os.environ['NUMBA_CACHE_DIR'] = '__pycache__'


def convert_pred_to_fasta(pred_csv, fasta_out_path, cutoff=400, col='epitope'):
    """
    Convert a prediction CSV file into a FASTA format file by extracting unique epitopes
    and writing them to a file.

    Parameters
    ----------
    pred_csv : str
        Path to the input CSV file containing columns including 'epitope'.
    fasta_out_path : str
        Path where the FASTA file will be saved, containing only unique epitopes.
    cutoff: int
        Exclude rows with epitopes that are generated by more than cutoff TCRs.
    """
    outdir = str(Path(pred_csv).parent.parent)
    name = str(Path(pred_csv).stem)
    # Load the DataFrame
    df = pd.read_csv(pred_csv)
    # Drop duplicates based on `col`
    unique_epitopes = list(set(df[col].tolist()))

    # Open a file to write the unique epitopes in FASTA format
    with open(fasta_out_path, 'w') as f:
        for epitope in unique_epitopes:
            header = f">{epitope}"
            f.write(f"{header}\n{epitope}\n")

    print(f"FASTA file created with unique epitopes at: {fasta_out_path}")


@measure_time
def run_blastp(pred_csv, outdir, blastp, db_path, col='epitope'):
    """
    Wrapper function to run blastp software on the epitopes in pred_csv.

    Parameters
    ----------
    pred_csv : str
        Path to the input CSV file.
    outdir : str
        Directory where the output files will be saved.
    blastp : str
        Path to the blastp executable.
    db_path : str
        Path to the blastp database.
    """
    Path(f'{outdir}/blastp_results').mkdir(parents=True, exist_ok=True)
    Path(f"{outdir}/fasta").mkdir(parents=True, exist_ok=True)
    name = str(Path(pred_csv).stem)

    fasta_out_path = f"{outdir}/fasta/{name}.fasta"
    convert_pred_to_fasta(pred_csv, fasta_out_path, col=col)

    # Ensure the output directory exists
    if not os.path.exists(outdir):
        os.makedirs(outdir)

    output_path = f"{outdir}/blastp_results/{name}.txt"
    cmd = [
        blastp,
        "-task", "blastp-short",
        "-query", fasta_out_path,
        "-db", db_path,
        "-out", output_path,
        "-outfmt", "6",
        "-evalue", "20000",
        "-max_target_seqs", "10",
        "-num_threads", "8"
    ]

    # Run the command
    print(f"Run blastp command for {pred_csv}. Use column={col}")
    result = subprocess.run(cmd, text=True, capture_output=True)

    if result.returncode == 0:
        print(f"BLASTp completed successfully. Output file created at: {output_path}")
    else:
        print("Error in running BLASTp:")
        print(result.stderr)

    return result


@measure_time
def create_EpiGen_table(pred_csv, blastp_result, blastdbcmd, outdir, use_mhc=False, col='pred_0'):
    # parsed_data:
    # blastp_result:
    Path(f"{outdir}/accessions").mkdir(parents=True, exist_ok=True)
    Path(f"{outdir}/tables").mkdir(parents=True, exist_ok=True)
    name = str(Path(pred_csv).stem)

    df = pd.read_csv(pred_csv)
    print(f"df was read. # of rows: {len(df)}")
    # Read blastp result and query the related protein and species
    df_blastp = pd.read_csv(
        blastp_result,
        sep='\t',
        names=[col, 'accession', 'match', 'match_len', 'remove', 'gap', 'epi_s', 'epi_e', 'prot_s', 'prot_e', 'evalue', 'score']
    )
    print(f"df_blastp was read. # of rows: {len(df_blastp)}")
    # Filter blastp result by min E-value
    idx = df_blastp.groupby([col])['evalue'].idxmin()
    df_blastp_filtered = df_blastp.loc[idx]
    print(f"df_blastp_filtered by E-value: total {len(df_blastp_filtered)} rows")
    df_blastp_filtered['accession'].to_csv(f"{outdir}/accessions/{name}.txt", index=False, header=False)
    print("Run blastdbcmd..")
    cmd = [
        blastdbcmd,  # path to bin/blastdbcmd
        "-db",
        "ncbi-blast-2.15.0+/db/swissprot",
        "-entry_batch",
        f"{outdir}/accessions/{name}.txt",
        "-outfmt",
        "%a %t"  # Get accession and title information
    ]
    # Run the command
    blastdbcmd_result = subprocess.run(cmd, text=True, capture_output=True)
    if blastdbcmd_result.returncode == 0:
        print(f"blastdbcmd completed successfully. ")
    else:
        print("Error in running BLASTp:")
        print(blastdbcmd_result.stderr)
    result = blastdbcmd_result.stdout.split("\n")
    print(f"Total entries in blastdbcmd_result: {len(result)}")

    # Remove accessions that were not queried
    result = [x for x in result if x.split(" ")[0] in df_blastp_filtered['accession'].tolist()]
    print(f"Total entries in blastdbcmd after filtering accessions: {len(result)}")
    prot_info_tuples = []
    for r in result:
        r_split = r.split(" ")
        accession = r_split[0]
        last_paren_idx = len(r) - 1 - r[::-1].index("[")
        try:
            protein = r[r.index("Full") + 5:r.index(";")]
            species = r[last_paren_idx:]
        except:
            protein = r[r.index("Full") + 5:last_paren_idx - 1]
            species = r[last_paren_idx:]
        prot_info_tuples.append((accession, protein, species))
    prot_df = pd.DataFrame(prot_info_tuples, columns=['accession', 'protein', 'species'])
    print(f"Total # of rows in prot_df: {len(prot_df)}")
    prot_df = prot_df.drop_duplicates(subset='accession', keep='first')
    print(f"Total # of rows in prot_df after duplicate removal: {len(prot_df)}")

    # Merge df, df_blastp_filtered, and prot_df
    df_merged = pd.merge(df, df_blastp_filtered, on=col, how='left').dropna(subset=['accession'])
    final_merged_df = pd.merge(df_merged, prot_df, on='accession', how='left')
    if use_mhc:
        EpiGen_table = final_merged_df[['tcr', 'mhc', col, 'accession', 'evalue', 'protein', 'species']]
    else:
        try:
            EpiGen_table = final_merged_df[['tcr', 'epitope', col, 'accession', 'evalue', 'protein', 'species']]
        except:  # No TCR
            EpiGen_table = final_merged_df(['epitope', col, 'accession', 'evalue', 'protein', 'species'])
    EpiGen_table.to_csv(f"{outdir}/tables/{name}.csv", index=False)
    print(f"*** {outdir}/tables/{name}.csv was created")


def merge_and_partition_epigen_tables(outdir, epigen_table_train, epigen_table_val, epigen_table_test, n_part=10):
    """
    Merges and partitions EpiGen tables from train, validation, and test sets.
    Processes only unique epitopes and partitions the data for slurm job submission.

    Parameters:
    -----------
    outdir : str
        Output directory path where partitioned files will be saved
    epigen_table_train : str
        Path to training set CSV file
    epigen_table_val : str
        Path to validation set CSV file
    epigen_table_test : str
        Path to test set CSV file
    n_part : int, optional
        Number of partitions to create (default: 10)

    Returns:
    --------
    list
        List of paths to the created partition files
    """
    # Create output directory if it doesn't exist
    os.makedirs(outdir, exist_ok=True)

    # Read the CSV files
    df_train = pd.read_csv(epigen_table_train)
    df_val = pd.read_csv(epigen_table_val)
    df_test = pd.read_csv(epigen_table_test)

    # Merge dataframes using concat
    df = pd.concat([df_train, df_val, df_test], axis=0, ignore_index=True)

    # Drop duplicates based on epitope column
    df = df.drop_duplicates(subset=['epitope'], keep='first')
    print(f"Total number of unique epitopes={len(df)}")

    # Calculate partition sizes
    N = len(df)
    partition_size = math.ceil(N / n_part)

    # List to store output file paths
    output_files = []

    # Partition df and save under outdir
    for i in range(n_part):
        start_idx = i * partition_size
        end_idx = min((i + 1) * partition_size, N)  # Ensure we don't exceed DataFrame length

        # Skip if start index is beyond DataFrame length
        if start_idx >= N:
            break

        output_file = os.path.join(outdir, f"EpiGen_table_part_{i}.csv")
        df.iloc[start_idx:end_idx].to_csv(output_file, index=False)
        output_files.append(output_file)
        print(f"Saved partition {i+1}/{n_part} to: {output_file}")

    return output_files


@measure_time
def identify_tumor_antigens(epigen_table, epi_db_path, outdir, col, threshold=0, method='substring', debug=None):
    """
    threshold: int
        th for `levenshtein` method. Does not matter when using `substring` method
    col: str
        pred_0 for prediction, epitope for dataset
    """
    from cancer_wu.analyze import process_predictions, find_matches
    assert col in ['pred_0', 'epitope']
    df = pd.read_csv(epigen_table)
    if debug:
        df = df.sample(n=debug)
    # Load the epitope database
    epi_db = pd.read_csv(epi_db_path)
    results = process_predictions(df[col], epi_db, threshold, method)
    df['match'], df['ref_epitope'], df['ref_protein'] = zip(*results)
    df['tumor_assoc'] = ((df['match'] == 1) & (df['species'] == '[Homo sapiens]')).astype(int)

    # Calculate statistics
    total_epitopes = len(df)
    matched_epitopes = df['match'].sum()
    human_epitopes = (df['species'] == '[Homo sapiens]').sum()
    tumor_associated = df['tumor_assoc'].sum()

    stats = {
        'total_epitopes': total_epitopes,
        'matched_epitopes': matched_epitopes,
        'matched_epitopes_percent': matched_epitopes / total_epitopes * 100,
        'human_epitopes': human_epitopes,
        'human_epitopes_percent': human_epitopes / total_epitopes * 100,
        'tumor_associated': tumor_associated,
        'tumor_associated_percent': tumor_associated / total_epitopes * 100,
    }

    # Save results
    Path(outdir).mkdir(parents=True, exist_ok=True)
    name = Path(epigen_table).stem
    output_path = os.path.join(outdir, f'{name}_tumor_antigen_annotated.csv')
    df.to_csv(output_path, index=False)

    # Print summary
    print("Statistical Summary of Epitopes")
    print("-" * 30)
    print(f"Total epitopes: {stats['total_epitopes']:,}")
    print(f"Matched epitopes: {stats['matched_epitopes']:,} ({stats['matched_epitopes_percent']:.1f}%)")
    print(f"Human-associated epitopes: {stats['human_epitopes']:,} ({stats['human_epitopes_percent']:.1f}%)")
    print(f"Tumor-associated epitopes: {stats['tumor_associated']:,} ({stats['tumor_associated_percent']:.1f}%)")
    print("-" * 30)
    print(f"\nResults saved to: {output_path}")

    return results


def retrieve_rows_tumor_associated(epigen_table, parted_tumor_antigen_annotation_root, col):
    """
    Retrieve rows from EpiGen table that are associated with tumor antigens.

    Parameters
    ----------
    epigen_table : str
        Path to the EpiGen table CSV file
    parted_tumor_antigen_annotation_root : str
        Directory containing partitioned tumor antigen annotation files
    col: str
        pred_0 for prediction, epitope for dataset

    Returns
    -------
    pd.DataFrame
        DataFrame containing tumor-associated rows
    """
    assert col in ['pred_0', 'epitope']
    # Read the main EpiGen table
    df = pd.read_csv(epigen_table)

    # Read all tumor antigen annotations
    df_tumor = []
    for file in os.listdir(parted_tumor_antigen_annotation_root):
        if file.endswith('.csv'):
            file_path = os.path.join(parted_tumor_antigen_annotation_root, file)
            df_tumor.append(pd.read_csv(file_path))

    # Combine all tumor antigen dataframes
    df_tumor = pd.concat(df_tumor, ignore_index=True)

    # Filter tumor-associated entries
    df_tumor = df_tumor[df_tumor['tumor_assoc'] == 1]
    tumor_epitopes = set(df_tumor[col].unique())

    # Filter main dataframe for tumor-associated epitopes
    df_tumor_associated = df[df[col].isin(tumor_epitopes)]

    # Create output directory and save results
    outdir = Path(epigen_table).parent / "partitions"
    outdir.mkdir(exist_ok=True)

    output_file = outdir / "tumor.csv"
    df_tumor_associated.to_csv(output_file, index=False)

    print(f"Total epitopes: {len(df)}")
    print(f"Tumor epitopes: {len(tumor_epitopes)}")
    print(f"Tumor-associated rows: {len(df_tumor_associated)}")
    print(f"Results saved to: {output_file}")

    return df_tumor_associated


def retrieve_rows_self_antigens(epigen_table, tumor_csv, col):
    """
    Identify self antigens from EpiGen Table that are from Homo sapiens but not tumor-associated.

    Parameters
    ----------
    epigen_table : str
        Path to the EpiGen table CSV file
    tumor_csv: str
        Path to the partitioned table that contains tumor-associated epitopes
    col: str
        pred_0 for prediction, epitope for dataset

    Returns
    -------
    pd.DataFrame
        DataFrame containing self antigens
    """
    # Read the main EpiGen table
    df = pd.read_csv(epigen_table)

    # Read all tumor antigen annotations
    df_tumor = pd.read_csv(tumor_csv)

    # Get unique tumor epitopes
    tumor_epitopes = set(df_tumor[col].unique())

    # Get the self antigens
    df_self = df[
        (df['species'] == '[Homo sapiens]') &
        (~df[col].isin(tumor_epitopes))
    ]

    # Create output directory and save results
    outdir = Path(epigen_table).parent / "partitions"
    outdir.mkdir(exist_ok=True)

    output_file = outdir / "self.csv"
    df_self.to_csv(output_file, index=False)

    print(f"Total epitopes: {len(df)}")
    print(f"Tumor epitopes: {len(tumor_epitopes)}")
    print(f"Self antigens: {len(df_self)}")
    print(f"Results saved to: {output_file}")

    return df_self


def accessions_list_from_table(epigen_table, tumor_csv, self_csv, col, desc):
    """
    Get accession numbers from EpiGen Table for epitopes that are neither
    self-antigens nor tumor-associated antigens.

    Parameters
    ----------
    epigen_table : str
        Path to the main EpiGen table CSV file
    tumor_csv : str
        Path to tumor antigens CSV file
    self_csv : str
        Path to self antigens CSV file
    desc : str
        Description to be added to output filename

    Returns
    -------
    list
        List of unique accession numbers
    """
    assert col in ['pred_0', 'epitope']
    # Read all tables
    df = pd.read_csv(epigen_table)
    df_tumor = pd.read_csv(tumor_csv)
    df_self = pd.read_csv(self_csv)

    # Get unique epitopes from tumor and self antigens
    epitopes_tumor = set(df_tumor[col].unique())
    epitopes_self = set(df_self[col].unique())
    epitopes_tumor_self = epitopes_tumor.union(epitopes_self)

    # Filter main dataframe to exclude tumor and self antigens
    df_filtered = df[~df[col].isin(epitopes_tumor_self)]

    # Get unique accession numbers
    accessions = sorted(list(set(df_filtered['accession'].dropna())))

    # Create output directory and save results
    outdir = Path(epigen_table).parent
    outdir.mkdir(exist_ok=True)

    # Create output filename with description
    outfile = outdir / f"accessions_list_{desc}.txt"

    # Write accessions to file
    with open(outfile, "w") as f:
        f.write("\n".join(accessions))

    print(f"Total epitopes: {len(df)}")
    print(f"Tumor epitopes: {len(epitopes_tumor)}")
    print(f"Self epitopes: {len(epitopes_self)}")
    print(f"Remaining epitopes: {len(df_filtered)}")
    print(f"Unique accessions: {len(accessions)}")
    print(f"Results saved to: {outfile}")

    return accessions


@measure_time
def accession2taxid(accession_list, desc, chunk_size=20000):
    """
    Convert accession numbers to taxids using NCBI E-utilities.
    Handles large files by partitioning into smaller chunks.

    Parameters
    ----------
    accession_list : str
        Path to file containing accession numbers
    desc : str
        Description for output files
    chunk_size : int
        Number of accessions to process per chunk

    Returns
    -------
    str
        Path to the combined results file
    """
    outdir = Path(accession_list).parent

    # Create temporary directory in current working directory
    temp_dir = Path(f"tmp_accession2taxid_{desc}")
    if temp_dir.exists():
        shutil.rmtree(temp_dir)
    temp_dir.mkdir()

    try:
        print(f"Created temporary directory: {temp_dir}")

        # Read and partition the accession list
        with open(accession_list) as f:
            accessions = f.read().strip().splitlines()

        # Calculate number of chunks
        n_chunks = (len(accessions) + chunk_size - 1) // chunk_size
        print(f"Processing {len(accessions)} accessions in {n_chunks} chunks")

        # Create partitioned files
        chunk_files = []
        for i in range(n_chunks):
            start_idx = i * chunk_size
            end_idx = min((i + 1) * chunk_size, len(accessions))
            chunk_accessions = accessions[start_idx:end_idx]

            chunk_file = temp_dir / f"chunk_{i}.txt"
            with open(chunk_file, 'w') as f:
                f.write('\n'.join(chunk_accessions))
            chunk_files.append(chunk_file)

        # Process each chunk
        result_files = []
        for i, chunk_file in enumerate(chunk_files):
            print(f"\nProcessing chunk {i+1}/{n_chunks}")
            result_file = temp_dir / f"result_chunk_{i}.txt"

            # Construct and execute command
            cmd = (f"cat {chunk_file} | "
                  f"epost -db protein | "
                  f"esummary -db protein | "
                  f"xtract -pattern DocumentSummary -element Caption,TaxId "
                  f"> {result_file}")

            try:
                result = subprocess.run(cmd, shell=True, text=True,
                                     capture_output=True)

                if result.returncode != 0:
                    print(f"Error in chunk {i}: {result.stderr}")
                    continue

                result_files.append(result_file)
                print(f"Completed chunk {i+1}")

            except Exception as e:
                print(f"Error processing chunk {i}: {str(e)}")
                continue

            # Add small delay to avoid overwhelming the NCBI server
            time.sleep(1)

        # Combine results
        final_output = outdir / f"result_{desc}.txt"
        with open(final_output, 'w') as outfile:
            for result_file in result_files:
                if result_file.exists():
                    with open(result_file) as infile:
                        outfile.write(infile.read())
                        outfile.write('\n')

        print(f"\nResults combined and saved to: {final_output}")
        print(f"Processed {len(result_files)}/{n_chunks} chunks successfully")

    finally:
        # Clean up temporary directory
        if temp_dir.exists():
            shutil.rmtree(temp_dir)
            print(f"Cleaned up temporary directory: {temp_dir}")

    return str(final_output)


def make_species2category(accessions_list, accession2tax_id_result, tax_id2category, epigen_table, outdir):
    # accessions_list: predictions/240620_all_data_topk32_th_100_train/tables/accession/others_accession.txt
    # accession2tax_id_result: result.txt
    # tax_id2category: tax_id2category.csv
    # epigen_table: predictions/240620_all_data_topk32_th_100_train/tables/240620_all_data_topk32_th_100_train.csv

    # 1. Create accession2species for the `others` category in the initial keyword-based categorization
    with open(accessions_list, "r") as f:
        accessions = f.readlines()
    accessions = [x.strip() for x in accessions]
    # accession 2 tax_id
    df_a2t = pd.read_csv(accession2tax_id_result, delimiter='\t', names=['accession', 'tax_id'])
    # tax_id 2 category
    df_t2c = pd.read_csv(tax_id2category)
    df_a2t = df_a2t.merge(df_t2c, on='tax_id')
    # original training set
    df_train = pd.read_csv(epigen_table)
    # Create a dataframe from the accessions list
    df_accessions = pd.DataFrame({'accession': accessions})

    # Merge df_accessions with df_train, keeping only the first occurrence of each accession
    accession2species = pd.merge(
        df_train[['accession', 'species']].drop_duplicates(subset='accession', keep='first'),
        df_accessions,
        on='accession',
        how='right'
    )

    # Reset the index
    accession2species = accession2species.reset_index(drop=True)
    accession2species['accession'] = accession2species['accession'].apply(lambda x: x.split(".")[0])
    accession2species = accession2species.merge(df_a2t, on='accession')

    Path(outdir).mkdir(parents=True, exist_ok=True)
    accession2species[['species', 'category']].to_csv(f"{outdir}/species2category.csv", index=False)
    print(f"{outdir}/species2category.csv")


def partition_data_by_category(epigen_table, species2category):
    outdir = str(Path(epigen_table).parent) + "/partitions"
    Path(outdir).mkdir(parents=True, exist_ok=True)
    df = pd.read_csv(epigen_table)
    species2category = pd.read_csv(species2category)

    # virus
    viral_species = species2category[species2category['category'] == 'virus']['species'].tolist()
    df[df['species'].isin(viral_species)].to_csv(f"{outdir}/virus.csv", index=False)
    print(f"{outdir}/virus.csv")

    # bacteria
    bacterial_species = species2category[species2category['category'] == 'bacteria']['species'].tolist()
    df[df['species'].isin(bacterial_species)].to_csv(f"{outdir}/bacteria.csv", index=False)
    print(f"{outdir}/bacteria.csv")

    # archaea
    archaea_species = species2category[species2category['category'] == 'archaea']['species'].tolist()
    df[df['species'].isin(archaea_species)].to_csv(f"{outdir}/archaea.csv", index=False)
    print(f"{outdir}/archaea.csv")

    # parasite
    parasite_species = species2category[species2category['category'] == 'parasite']['species'].tolist()
    df[df['species'].isin(parasite_species)].to_csv(f"{outdir}/parasite.csv", index=False)
    print(f"{outdir}/parasite.csv")

    # fungi
    fungal_species = species2category[species2category['category'] == 'fungi']['species'].tolist()
    df[df['species'].isin(fungal_species)].to_csv(f"{outdir}/fungi.csv", index=False)
    print(f"{outdir}/fungi.csv")

    # Eukaryota
    eukaryotes_species = species2category[species2category['category'] == 'eukaryota']['species'].tolist()
    df[df['species'].isin(eukaryotes_species)].to_csv(f"{outdir}/eukaryota.csv", index=False)
    print(f"{outdir}/eukaryota.csv")


def add_category_annotation(epigen_table, species2category, tumor_csv, self_csv, col):
    """
    Add category annotations to a table based on species information and epitope matches.

    Args:
        epigen_table (str): Path to the input CSV file containing epigenetic data with 'species' and 'epitope' columns
        species2category (str): Path to the CSV file containing species to category mappings
        tumor_csv (str): Path to CSV file containing tumor-associated epitopes
        self_csv (str): Path to CSV file containing self-associated epitopes

    Returns:
        pd.DataFrame: DataFrame with added category annotations including 'tumor' and 'self' designations
    """
    assert col in ['pred_0', 'epitope']
    # Read input files
    df = pd.read_csv(epigen_table)
    spec2cat = pd.read_csv(species2category)
    df_tumor = pd.read_csv(tumor_csv)
    epitopes_tumor = set(df_tumor[col].unique())
    df_self = pd.read_csv(self_csv)
    epitopes_self = set(df_self[col].unique())

    # Create mapping dictionary and add categories
    category_map = dict(zip(spec2cat['species'], spec2cat['category']))
    df['category'] = df['species'].map(category_map)

    # Create mask for tumor and self matches
    tumor_mask = df[col].isin(epitopes_tumor)
    self_mask = df[col].isin(epitopes_self)

    # Update categories based on epitope matches
    # Priority: if an epitope is both tumor and self, mark as tumor
    df.loc[tumor_mask, 'category'] = 'tumor'
    df.loc[self_mask & ~tumor_mask, 'category'] = 'self'

    # Add additional columns for analysis if needed
    df['is_tumor'] = tumor_mask
    df['is_self'] = self_mask

    # Save results
    outdir = Path(epigen_table).parent
    name = Path(epigen_table).stem
    output_path = outdir / f"{name}_cat_annotated.csv"
    df.to_csv(output_path, index=False)
    print(f"Saved annotated file to: {output_path}")

    # Print summary statistics
    print("\nCategory Distribution:")
    print(df['category'].value_counts())
    print(f"\nTotal tumor epitopes: {tumor_mask.sum()}")
    print(f"Total self epitopes: {self_mask.sum()}")
    print(f"Epitopes that are both tumor and self: {(tumor_mask & self_mask).sum()}")

    return df


def count_rows(data_dir):
    # Count the number of rows of csv files under data_dir
    for x in os.listdir(data_dir):
        df = pd.read_csv(f"{data_dir}/{x}")
        print(f"{x}: {len(df)}")


def construct_balanced_data(outdir: str, annotated_table: str, seed: int = 42) -> pd.DataFrame:
    """
    Constructs a balanced dataset according to specified category ratio constraints.

    Args:
        annotated_table (str): Path to CSV file containing annotated data with 'category' column
        seed (int): Random seed for reproducibility

    Returns:
        pd.DataFrame: Balanced dataset satisfying the specified constraints
    """
    # Set random seeds for reproducibility
    random.seed(seed)
    np.random.seed(seed)

    constraints = {
        "viral_dominance": lambda dist: dist["virus"] > 0.50,
        "endogenous_presence_self": lambda dist: 0.03 <= dist["self"] <= 0.15,
        "endogenous_presence_tumor": lambda dist: 0.01 <= dist["tumor"] <= 0.05,
        "bacterial_limit": lambda dist: dist["bacteria"] <= dist["virus"] / 5,
        "archaea_limit": lambda dist: dist["archaea"] <= 0.01,
        "fungi_parasite_limit": lambda dist: dist["fungi"] + dist["parasite"] <= dist["virus"] / 10
    }

    def sample_ratios_satisfying_constraints(constraints: Dict[str, Callable]) -> Dict[str, float]:
        """
        Generate random ratios that satisfy all constraints using rejection sampling.
        First samples virus ratio, then directly samples other ratios within their constraints.
        """
        max_attempts = 1000
        categories = ["virus", "self", "tumor", "bacteria", "archaea", "parasite", "fungi"]

        for attempt in range(max_attempts):
            # Sample virus ratio (must be > 0.50)
            ratios = {"virus": random.uniform(0.50, 0.80)}

            # Sample remaining ratios
            remaining = 1 - ratios["virus"]
            try:
                # Sample self (0.03 <= self <= 0.15)
                ratios["self"] = random.uniform(0.03, min(0.15, remaining))
                remaining -= ratios["self"]

                # Sample tumor (0.01 <= tumor <= 0.05)
                ratios["tumor"] = random.uniform(0.01, min(0.05, remaining))
                remaining -= ratios["tumor"]

                # Sample bacteria (<= virus/5)
                max_bacteria = min(ratios["virus"] / 5, remaining)
                ratios["bacteria"] = random.uniform(0, max_bacteria)
                remaining -= ratios["bacteria"]

                # Sample archaea (<= 0.01)
                ratios["archaea"] = random.uniform(0, min(0.01, remaining))
                remaining -= ratios["archaea"]

                # Sample fungi and parasite (sum <= virus/10)
                max_fungi_parasite = min(ratios["virus"] / 10, remaining)
                if max_fungi_parasite > 0:
                    # Split remaining fungi+parasite budget randomly
                    fungi_ratio = random.random()
                    ratios["fungi"] = max_fungi_parasite * fungi_ratio
                    ratios["parasite"] = max_fungi_parasite * (1 - fungi_ratio)
                    remaining -= (ratios["fungi"] + ratios["parasite"])
                else:
                    ratios["fungi"] = 0
                    ratios["parasite"] = 0

                # Check if all constraints are satisfied
                if all(constraint(ratios) for constraint in constraints.values()):
                    # Normalize to ensure sum is exactly 1
                    total = sum(ratios.values())
                    ratios = {k: v/total for k, v in ratios.items()}
                    return ratios

            except ValueError:
                continue

        raise ValueError("Could not find valid ratios after maximum attempts")

    # Sample target ratios
    try:
        ratios = sample_ratios_satisfying_constraints(constraints)
        print("Selected category ratios:")
        for cat, ratio in ratios.items():
            print(f"{cat}: {ratio:.3f}")
    except ValueError as e:
        print(f"Error: {e}")
        return None

    # Read data
    df = pd.read_csv(annotated_table)

    # Calculate target total number based on virus count
    virus_count = len(df[df['category'] == 'virus'])
    target_total = int(virus_count / ratios['virus'])

    # Calculate target numbers for each category
    cat2num = {k: int(target_total * v) for k, v in ratios.items()}

    # Sample from each category
    sampled_dfs = []
    for cat, num in cat2num.items():
        category_df = df[df['category'] == cat]

        if len(category_df) < num:
            print(f"Warning: Not enough samples for {cat}. Need {num}, have {len(category_df)}")
            sampled = category_df  # Take all available samples
        else:
            sampled = category_df.sample(n=num, random_state=seed)

        sampled_dfs.append(sampled)

    # Combine all sampled data
    final_df = pd.concat(sampled_dfs, ignore_index=True)

    # Save results
    # outdir = Path(annotated_table).parent
    Path(outdir).mkdir(parents=True, exist_ok=True)
    name = Path(annotated_table).stem
    # output_path = outdir / f"{name}_balanced.csv"
    output_path = f"{outdir}/{name}_balanced.csv"
    final_df.to_csv(output_path, index=False)

    # Print summary statistics
    print("\nFinal category distribution:")
    category_counts = final_df['category'].value_counts()
    total_samples = len(final_df)
    for category, count in category_counts.items():
        percentage = (count / total_samples) * 100
        print(f"{category}: {count} samples ({percentage:.1f}%)")

    print(f"\nSaved balanced dataset to: {output_path}")

    # Save only tcr and epitope
    final_df = final_df.rename(columns={'tcr': 'text', 'epitope': 'label'})
    final_df[['text', 'label']].to_csv(f"{outdir}/{name}_formatted.csv", index=False)
    print(f"{outdir}/{name}_formatted.csv")

    return final_df


def visualize_antigen_category_dist(epigen_table_annotated, outdir):
    """
    Visualize antigen category distribution from annotated EpiGen table
    """
    category_order = ["Virus", "Bacteria", "Self", "Tumor", "Parasite", "Fungi", "Archaea", "Others"]
    # Define colors for each category
    category_colors = {
        "Virus": "#FF7F0E",     # Orange
        "Bacteria": "#1F77B4",  # Blue
        "Self": "#2CA02C",      # Green
        "Tumor": "#D62728",     # Red
        "Parasite": "#9467BD",  # Purple
        "Fungi": "#8C564B",     # Brown
        "Archaea": "#E377C2",   # Pink
        "Others": "#7F7F7F"     # Gray
    }

    # Get category counts and handle merging
    epigen_table = pd.read_csv(epigen_table_annotated)
    epigen_table['category'] = epigen_table['category'].str.capitalize()  # Capitalize first letter

    # Replace categories not in category_order with 'Others'
    valid_categories = set(category_order)
    epigen_table.loc[~epigen_table['category'].isin(valid_categories), 'category'] = 'Others'

    cat2num = dict(epigen_table.groupby('category').count()['species'])
    cat2num['Virus'] = cat2num.get('Virus', 0) + cat2num.pop('Retrovirus', 0) + cat2num.pop('Bacteriophage', 0)
    # Create DataFrame with ordered categories
    df = pd.DataFrame([{'Category': cat, 'Count': cat2num.get(cat, 0)} for cat in category_order])
    df['Percentage'] = (df['Count'] / df['Count'].sum() * 100).round(1)

    # Create and save plot
    plt.figure(figsize=(10, 6))
    bars = sns.barplot(x='Count', y='Category', data=df, order=category_order,
                      palette=[category_colors[cat] for cat in category_order])

    # Add count and percentage labels
    for i, (count, pct) in enumerate(zip(df['Count'], df['Percentage'])):
        if count > 0:
            plt.text(count, i, f' {count:,} ({pct}%)', va='center')

    plt.title('Distribution of Antigen Categories')
    plt.grid(True, axis='x', alpha=0.3)
    plt.tight_layout()

    # Save plot
    Path(outdir).mkdir(parents=True, exist_ok=True)
    output_path = f"{outdir}/pred_category_dist.pdf"
    plt.savefig(output_path, bbox_inches='tight')
    print(f"Plot saved to: {output_path}")
    print(df.to_string(index=False))

    return cat2num


def run_efetch_parallel(tax_ids_file, output_dir, chunk_size=50):
    # Create output directory if it doesn't exist
    Path(output_dir).mkdir(parents=True, exist_ok=True)

    # Read the tax_ids file
    df = pd.read_csv(tax_ids_file, delimiter='\t', names=['accession', 'tax_id'])
    tax_ids = df['tax_id'].astype(str).tolist()

    # Partition tax_ids into chunks
    tax_id_chunks = np.array_split(tax_ids, np.ceil(len(tax_ids) / chunk_size))

    total_processed = 0

    for chunk in tqdm(tax_id_chunks, desc="Processing chunks"):
        chunk_ids = ",".join(chunk)

        # Construct and run the efetch command for this chunk
        cmd = f"efetch -db taxonomy -id {chunk_ids} -format xml"
        process = Popen(cmd, shell=True, stdout=PIPE, stderr=PIPE)
        stdout, stderr = process.communicate()

        if process.returncode != 0:
            print(f"Error running efetch for chunk: {stderr.decode()}")
            continue

        # Parse the XML string
        try:
            root = ET.fromstring(stdout)
        except ET.ParseError as e:
            print(f"Error parsing XML for chunk: {e}")
            continue

        # Save individual TaxaSet elements to separate files
        # for taxon in root.findall('.//TaxaSet'):
        for taxon in root:
            tax_id = taxon[0].text
            filename = os.path.join(output_dir, f"taxon_{tax_id}.xml")

            # Create a new XML tree with this TaxaSet as root
            new_root = ET.Element("TaxaSet")
            new_root.append(taxon)
            tree = ET.ElementTree(new_root)

            # Write the XML to file
            tree.write(filename, encoding="utf-8", xml_declaration=True)

            total_processed += 1

    print(f"Processed {total_processed} taxa in total")


def parse_efetch_result(xml_file):
    # Taxonomic classification dictionaries
    TAXONOMIC_GROUPS = {
        'virus': {
            'families': {'viridae', 'virus'},
            'special_cases': {'hiv', 'immunodeficiency', 'sars', 'coronavirus'},
            'exclusions': {'bacteriophage', 'provirus'},  # Cases that need special handling
            'retro_markers': {'retroviridae', 'retrovirus'},
            'phage_families': {'caudovirales', 'myoviridae', 'siphoviridae', 'podoviridae'}
        },
        'parasite': {
            'genera': {
                'Plasmodium', 'Trypanosoma', 'Leishmania', 'Toxoplasma', 'Cryptosporidium',
                'Giardia', 'Entamoeba', 'Trichomonas', 'Babesia', 'Theileria',
                'Schistosoma', 'Fasciola', 'Taenia', 'Echinococcus', 'Ascaris',
                'Trichinella', 'Wuchereria', 'Onchocerca', 'Brugia', 'Loa',
                'Trichuris', 'Ancylostoma', 'Necator', 'Strongyloides',
                'Pediculus', 'Sarcoptes', 'Ixodes', 'Dermacentor'
            },
            'families': {
                'Plasmodiidae', 'Trypanosomatidae', 'Toxoplasmatidae', 'Cryptosporidiidae',
                'Giardiidae', 'Entamoebidae', 'Trichomonadidae', 'Babesiidae', 'Theileriidae',
                'Schistosomatidae', 'Fasciolidae', 'Taeniidae', 'Ascarididae',
                'Trichinellidae', 'Onchocercidae', 'Trichuridae', 'Ancylostomatidae',
                'Strongyloididae', 'Pediculidae', 'Sarcoptidae', 'Ixodidae'
            },
            'phyla': {
                'Apicomplexa', 'Platyhelminthes', 'Nematoda', 'Arthropoda'
            },
            'protozoa_phyla': {
                'apicomplexa', 'ciliophora', 'euglenozoa', 'amoebozoa', 'microsporidia'
            }
        }
    }

    def extract_lineage(xml_root):
        """Extract lineage information from XML root."""
        lineage = defaultdict(list)
        try:
            lineage_element = xml_root.find('.//LineageEx')
            if lineage_element is not None:
                for taxon in lineage_element:
                    tax_id = taxon.find('TaxId').text
                    name = taxon.find('ScientificName').text.lower()
                    rank = taxon.find('Rank').text
                    lineage[rank].append(name)
            return lineage
        except Exception as e:
            logger.error(f"Error extracting lineage: {e}")
            return lineage

    def contains_any(strings, substrings):
        """Check if any string in a list contains any of the substrings."""
        return any(any(sub.lower() in s.lower() for s in strings) for sub in substrings)

    def is_virus(lineage, scientific_name):
        """Determine if the organism is a virus."""
        virus_info = TAXONOMIC_GROUPS['virus']

        # Check for virus families and special cases
        if (contains_any(lineage['family'] + lineage['genus'], virus_info['families']) or
            any(marker in scientific_name.lower() for marker in virus_info['special_cases'])):

            # Handle retroviruses
            if contains_any(lineage['family'] + lineage['genus'], virus_info['retro_markers']):
                return True

            # Handle bacteriophages
            if contains_any(lineage['family'] + lineage['genus'], virus_info['phage_families']):
                return True

            # Exclude certain cases
            if not any(excl in scientific_name.lower() for excl in virus_info['exclusions']):
                return True

        return False

    def is_parasite(lineage):
        """Determine if the organism is a parasite."""
        parasite_info = TAXONOMIC_GROUPS['parasite']

        # Check protozoa
        if contains_any(lineage['phylum'], parasite_info['protozoa_phyla']):
            return True

        # Check specific genera and families
        if (len(lineage['genus']) > 0 and lineage['genus'][0] in parasite_info['genera'] or
            len(lineage['family']) > 0 and lineage['family'][0] in parasite_info['families']):
            return True

        # Check parasitic phyla
        if len(lineage['phylum']) > 0 and lineage['phylum'][0] in parasite_info['phyla']:
            return True

        return False

    try:
        tree = ET.parse(xml_file)
        xml = tree.getroot()
        lineage = extract_lineage(xml)
        scientific_name = xml.find('.//ScientificName').text.lower()

        # Check for viruses first
        if is_virus(lineage, scientific_name):
            return 'virus'

        # Process based on superkingdom
        if lineage['superkingdom']:
            superkingdom = lineage['superkingdom'][0].lower()

            if superkingdom == 'bacteria':
                return 'bacteria'
            elif superkingdom == 'archaea':
                return 'archaea'
            elif superkingdom == 'eukaryota':
                # Check for fungi
                if len(lineage['kingdom']) > 0 and any(k in lineage['kingdom'][0].lower() for k in ['fungi', 'mycota']):
                    return 'fungi'

                # Check for parasites
                if is_parasite(lineage):
                    return 'parasite'

                return 'eukaryota'

        return 'others'

    except Exception as e:
        logger.error(f"Error parsing taxonomy file: {e}")
        return 'others'
